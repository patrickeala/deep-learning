{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                40970     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 929,802\n",
      "Trainable params: 929,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.6119 - acc: 0.4092 - val_loss: 1.2588 - val_acc: 0.5514\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.2806 - acc: 0.5419 - val_loss: 1.0468 - val_acc: 0.6311\n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.1667 - acc: 0.5871 - val_loss: 1.0079 - val_acc: 0.6430\n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 1.0902 - acc: 0.6186 - val_loss: 0.8925 - val_acc: 0.6944\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 1.0423 - acc: 0.6340 - val_loss: 0.9215 - val_acc: 0.6825\n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 1.0032 - acc: 0.6479 - val_loss: 0.9115 - val_acc: 0.6827\n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.9841 - acc: 0.6561 - val_loss: 0.8277 - val_acc: 0.7149\n",
      "Epoch 8/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.9560 - acc: 0.6646 - val_loss: 0.8073 - val_acc: 0.7193\n",
      "Epoch 9/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.9385 - acc: 0.6729 - val_loss: 0.8059 - val_acc: 0.7182\n",
      "Epoch 10/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.9206 - acc: 0.6788 - val_loss: 0.8031 - val_acc: 0.7253\n",
      "Epoch 11/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.9070 - acc: 0.6843 - val_loss: 0.7753 - val_acc: 0.7364\n",
      "Epoch 12/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.9017 - acc: 0.6853 - val_loss: 0.7244 - val_acc: 0.7544\n",
      "Epoch 13/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8843 - acc: 0.6925 - val_loss: 0.7732 - val_acc: 0.7397\n",
      "Epoch 14/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8800 - acc: 0.6930 - val_loss: 0.7623 - val_acc: 0.7390\n",
      "Epoch 15/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8738 - acc: 0.6930 - val_loss: 0.7209 - val_acc: 0.7560\n",
      "Epoch 16/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8638 - acc: 0.7014 - val_loss: 0.7655 - val_acc: 0.7446\n",
      "Epoch 17/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8590 - acc: 0.7022 - val_loss: 0.7346 - val_acc: 0.7519\n",
      "Epoch 18/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8494 - acc: 0.7041 - val_loss: 0.7385 - val_acc: 0.7476\n",
      "Epoch 19/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8424 - acc: 0.7082 - val_loss: 0.7103 - val_acc: 0.7617\n",
      "Epoch 20/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8475 - acc: 0.7055 - val_loss: 0.6987 - val_acc: 0.7601\n",
      "Epoch 21/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8374 - acc: 0.7116 - val_loss: 0.7091 - val_acc: 0.7610\n",
      "Epoch 22/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.8284 - acc: 0.7132 - val_loss: 0.7270 - val_acc: 0.7550\n",
      "Epoch 23/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8258 - acc: 0.7104 - val_loss: 0.6761 - val_acc: 0.7703\n",
      "Epoch 24/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8164 - acc: 0.7202 - val_loss: 0.6897 - val_acc: 0.7666\n",
      "Epoch 25/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8217 - acc: 0.7161 - val_loss: 0.6613 - val_acc: 0.7753\n",
      "Epoch 26/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8114 - acc: 0.7193 - val_loss: 0.7457 - val_acc: 0.7508\n",
      "Epoch 27/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8063 - acc: 0.7212 - val_loss: 0.6767 - val_acc: 0.7729\n",
      "Epoch 28/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8069 - acc: 0.7232 - val_loss: 0.6817 - val_acc: 0.7706\n",
      "Epoch 29/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.8045 - acc: 0.7207 - val_loss: 0.7491 - val_acc: 0.7496\n",
      "Epoch 30/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7989 - acc: 0.7256 - val_loss: 0.7247 - val_acc: 0.7616\n",
      "Epoch 31/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7915 - acc: 0.7259 - val_loss: 0.6359 - val_acc: 0.7854\n",
      "Epoch 32/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7934 - acc: 0.7277 - val_loss: 0.6486 - val_acc: 0.7798\n",
      "Epoch 33/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7960 - acc: 0.7256 - val_loss: 0.7614 - val_acc: 0.7481\n",
      "Epoch 34/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7895 - acc: 0.7260 - val_loss: 0.6680 - val_acc: 0.7732\n",
      "Epoch 35/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7798 - acc: 0.7331 - val_loss: 0.7543 - val_acc: 0.7484\n",
      "Epoch 36/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7790 - acc: 0.7324 - val_loss: 0.6693 - val_acc: 0.7779\n",
      "Epoch 37/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7848 - acc: 0.7257 - val_loss: 0.6958 - val_acc: 0.7653\n",
      "Epoch 38/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7775 - acc: 0.7329 - val_loss: 0.6894 - val_acc: 0.7692\n",
      "Epoch 39/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7816 - acc: 0.7304 - val_loss: 0.7913 - val_acc: 0.7431\n",
      "Epoch 40/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7748 - acc: 0.7350 - val_loss: 0.6786 - val_acc: 0.7686\n",
      "Epoch 41/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7748 - acc: 0.7344 - val_loss: 0.7084 - val_acc: 0.7642\n",
      "Epoch 42/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7668 - acc: 0.7354 - val_loss: 0.6891 - val_acc: 0.7722\n",
      "Epoch 43/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7706 - acc: 0.7342 - val_loss: 0.6578 - val_acc: 0.7759\n",
      "Epoch 44/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7676 - acc: 0.7368 - val_loss: 0.6954 - val_acc: 0.7697\n",
      "Epoch 45/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7660 - acc: 0.7354 - val_loss: 0.6804 - val_acc: 0.7753\n",
      "Epoch 46/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7680 - acc: 0.7364 - val_loss: 0.6802 - val_acc: 0.7760\n",
      "Epoch 47/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7611 - acc: 0.7380 - val_loss: 0.7523 - val_acc: 0.7531\n",
      "Epoch 48/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7651 - acc: 0.7368 - val_loss: 0.6736 - val_acc: 0.7770\n",
      "Epoch 49/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7645 - acc: 0.7372 - val_loss: 0.6824 - val_acc: 0.7731\n",
      "Epoch 50/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7617 - acc: 0.7388 - val_loss: 0.6758 - val_acc: 0.7747\n",
      "Epoch 51/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7636 - acc: 0.7383 - val_loss: 0.6663 - val_acc: 0.7770\n",
      "Epoch 52/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7632 - acc: 0.7364 - val_loss: 0.6520 - val_acc: 0.7860\n",
      "Epoch 53/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7608 - acc: 0.7370 - val_loss: 0.6295 - val_acc: 0.7902\n",
      "Epoch 54/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7552 - acc: 0.7418 - val_loss: 0.6649 - val_acc: 0.7826\n",
      "Epoch 55/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7538 - acc: 0.7420 - val_loss: 0.6888 - val_acc: 0.7729\n",
      "Epoch 56/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7547 - acc: 0.7391 - val_loss: 0.6226 - val_acc: 0.7901\n",
      "Epoch 57/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7465 - acc: 0.7423 - val_loss: 0.6557 - val_acc: 0.7808\n",
      "Epoch 58/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7494 - acc: 0.7439 - val_loss: 0.6261 - val_acc: 0.7881\n",
      "Epoch 59/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7452 - acc: 0.7445 - val_loss: 0.5980 - val_acc: 0.7997\n",
      "Epoch 60/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7481 - acc: 0.7410 - val_loss: 0.6403 - val_acc: 0.7896\n",
      "Epoch 61/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7412 - acc: 0.7452 - val_loss: 0.6504 - val_acc: 0.7836\n",
      "Epoch 62/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7413 - acc: 0.7457 - val_loss: 0.6504 - val_acc: 0.7809\n",
      "Epoch 63/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7387 - acc: 0.7484 - val_loss: 0.6393 - val_acc: 0.7916\n",
      "Epoch 64/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7448 - acc: 0.7439 - val_loss: 0.6356 - val_acc: 0.7873\n",
      "Epoch 65/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7350 - acc: 0.7467 - val_loss: 0.7459 - val_acc: 0.7594\n",
      "Epoch 66/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7493 - acc: 0.7416 - val_loss: 0.6333 - val_acc: 0.7887\n",
      "Epoch 67/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7348 - acc: 0.7483 - val_loss: 0.6467 - val_acc: 0.7854\n",
      "Epoch 68/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7313 - acc: 0.7481 - val_loss: 0.6461 - val_acc: 0.7879\n",
      "Epoch 69/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7392 - acc: 0.7471 - val_loss: 0.6925 - val_acc: 0.7711\n",
      "Epoch 70/100\n",
      "1562/1562 [==============================] - 20s 12ms/step - loss: 0.7305 - acc: 0.7500 - val_loss: 0.6783 - val_acc: 0.7760\n",
      "Epoch 71/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7409 - acc: 0.7462 - val_loss: 0.6285 - val_acc: 0.7879\n",
      "Epoch 72/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7340 - acc: 0.7478 - val_loss: 0.6274 - val_acc: 0.7913\n",
      "Epoch 73/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7381 - acc: 0.7475 - val_loss: 0.6696 - val_acc: 0.7778\n",
      "Epoch 74/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7293 - acc: 0.7519 - val_loss: 0.6596 - val_acc: 0.7812\n",
      "Epoch 75/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.7342 - acc: 0.7506 - val_loss: 0.6040 - val_acc: 0.7982\n",
      "Epoch 76/100\n",
      "1562/1562 [==============================] - 18s 12ms/step - loss: 0.7299 - acc: 0.7487 - val_loss: 0.6549 - val_acc: 0.7865\n",
      "Epoch 77/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7294 - acc: 0.7495 - val_loss: 0.6614 - val_acc: 0.7795\n",
      "Epoch 78/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7246 - acc: 0.7519 - val_loss: 0.6679 - val_acc: 0.7840\n",
      "Epoch 79/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7381 - acc: 0.7477 - val_loss: 0.6266 - val_acc: 0.7935\n",
      "Epoch 80/100\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.7277 - acc: 0.7521 - val_loss: 0.6242 - val_acc: 0.7910\n",
      "Epoch 81/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7327 - acc: 0.7461 - val_loss: 0.7230 - val_acc: 0.7645\n",
      "Epoch 82/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7347 - acc: 0.7470 - val_loss: 0.6374 - val_acc: 0.7883\n",
      "Epoch 83/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7257 - acc: 0.7526 - val_loss: 0.6856 - val_acc: 0.7784\n",
      "Epoch 84/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7253 - acc: 0.7517 - val_loss: 0.6105 - val_acc: 0.7973\n",
      "Epoch 85/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7364 - acc: 0.7475 - val_loss: 0.6416 - val_acc: 0.7870\n",
      "Epoch 86/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7207 - acc: 0.7526 - val_loss: 0.6529 - val_acc: 0.7850\n",
      "Epoch 87/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7188 - acc: 0.7535 - val_loss: 0.6518 - val_acc: 0.7859\n",
      "Epoch 88/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7196 - acc: 0.7529 - val_loss: 0.6422 - val_acc: 0.7881\n",
      "Epoch 89/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7225 - acc: 0.7528 - val_loss: 0.7840 - val_acc: 0.7503\n",
      "Epoch 90/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7206 - acc: 0.7538 - val_loss: 0.6791 - val_acc: 0.7812\n",
      "Epoch 91/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7235 - acc: 0.7527 - val_loss: 0.6330 - val_acc: 0.7937\n",
      "Epoch 92/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7231 - acc: 0.7542 - val_loss: 0.6646 - val_acc: 0.7802\n",
      "Epoch 93/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7162 - acc: 0.7554 - val_loss: 0.6248 - val_acc: 0.7912\n",
      "Epoch 94/100\n",
      "1562/1562 [==============================] - 21s 13ms/step - loss: 0.7222 - acc: 0.7523 - val_loss: 0.7712 - val_acc: 0.7559\n",
      "Epoch 95/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7187 - acc: 0.7565 - val_loss: 0.6179 - val_acc: 0.7948\n",
      "Epoch 96/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7276 - acc: 0.7531 - val_loss: 0.6002 - val_acc: 0.7977\n",
      "Epoch 97/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7205 - acc: 0.7523 - val_loss: 0.6329 - val_acc: 0.7865\n",
      "Epoch 98/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7162 - acc: 0.7566 - val_loss: 0.6937 - val_acc: 0.7763\n",
      "Epoch 99/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7156 - acc: 0.7559 - val_loss: 0.6504 - val_acc: 0.7814\n",
      "Epoch 100/100\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.7239 - acc: 0.7538 - val_loss: 0.6372 - val_acc: 0.7835\n",
      "10000/10000 [==============================] - 1s 124us/step\n",
      "Test loss: 0.6372010903358459\n",
      "Test accuracy: 0.7835\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "# Loading the CIFAR-10 datasets\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# define variables\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "num_predictions = 20\n",
    "\n",
    "\n",
    "# Train - Test splitting of data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Conversion to binary matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Hyperparameters for CNN\n",
    "filters = 128\n",
    "kernel_size = 3\n",
    "activation = 'relu'\n",
    "input_shape = (x_train.shape[1], x_train.shape[1], 3)\n",
    "max_pool = 2\n",
    "num_labels = 10\n",
    "dropout = 0.3\n",
    "\n",
    "# Define CNN Layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = filters,\n",
    "                kernel_size = kernel_size,\n",
    "                input_shape = input_shape))\n",
    "model.add(Activation(activation))                \n",
    "model.add(MaxPooling2D(max_pool))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Conv2D(filters = filters*2,\n",
    "                kernel_size = kernel_size))\n",
    "model.add(Activation(activation))                \n",
    "model.add(MaxPooling2D(max_pool))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Conv2D(filters = filters*2,\n",
    "                kernel_size = kernel_size))\n",
    "model.add(Activation(activation))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Training using RMSProp\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data Augmentation (data augmentation code taken from github)\n",
    "data_augmentation = True\n",
    "\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4,\n",
    "                        steps_per_epoch = len(x_train)// batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# Print loss and accuracy\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
